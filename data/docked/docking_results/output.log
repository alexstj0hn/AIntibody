Running job roahi

Try to find the ccd cache data in the code directory for inference.
[2025-01-29 21:44:35,981] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-01-29 21:44:51,183 [/home/ubuntu/Protenix/runner/inference.py:152] INFO __main__: Distributed environment: world size: 1, global rank: 0, local rank: 0
2025-01-29 21:44:51,184 [/home/ubuntu/Protenix/runner/inference.py:62] INFO root: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
2025-01-29 21:44:51,184 [/home/ubuntu/Protenix/runner/inference.py:82] INFO root: The kernels will be compiled when fast_layernorm is called for the first time.
2025-01-29 21:44:51,184 [/home/ubuntu/Protenix/runner/inference.py:86] INFO root: Finished init ENV.
2025-01-29 21:44:57,888 [/home/ubuntu/Protenix/runner/inference.py:152] INFO __main__: Loading from /usr/local/lib/python3.10/dist-packages/./release_data/checkpoint/model_v0.2.0.pt, strict: False
2025-01-29 21:49:04,911 [/home/ubuntu/Protenix/runner/inference.py:152] INFO __main__: Sampled key: module.input_embedder.atom_attention_encoder.linear_no_bias_f.weight
2025-01-29 21:49:05,083 [/home/ubuntu/Protenix/runner/inference.py:152] INFO __main__: Finish loading checkpoint.
2025-01-29 21:49:05,094 [/home/ubuntu/Protenix/runner/inference.py:204] INFO __main__: Loading data from
input.json
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
train scheduler 16.0
inference scheduler 16.0
Diffusion Module has 16.0
2025-01-29 21:49:05,873 [/usr/local/lib/python3.10/dist-packages/protenix/data/infer_data_pipeline.py:209] INFO protenix.data.infer_data_pipeline: Featurizing roahi...
2025-01-29 21:50:54,469 [/home/ubuntu/Protenix/runner/inference.py:224] INFO __main__: [Rank 0 (1/1)] roahi: N_asym 3, N_token 453, N_atom 3542, N_msa 14350
2025-01-29 21:58:36,172 [/home/ubuntu/Protenix/runner/inference.py:243] INFO __main__: [Rank 0] roahi succeeded.
Results saved to out
